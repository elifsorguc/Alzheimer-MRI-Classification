{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess dataset (adjust dataset path accordingly)\n",
    "def load_dataset(base_path, target_size=(128, 128)):\n",
    "    classes = ['Non_Demented', 'Very_Mild_Demented', 'Mild_Demented', 'Moderate_Demented']\n",
    "    images, labels = [], []\n",
    "    for idx, label in enumerate(classes):\n",
    "        class_path = os.path.join(base_path, label)\n",
    "        for file in os.listdir(class_path):\n",
    "            image = cv2.imread(os.path.join(class_path, file))\n",
    "            if image is not None:\n",
    "                image = cv2.resize(image, target_size)\n",
    "                images.append(image)\n",
    "                labels.append(idx)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Assuming the dataset is stored in /content/dataset\n",
    "base_path = '/content/dataset'\n",
    "images, labels = load_dataset(base_path)\n",
    "\n",
    "# Normalize the data\n",
    "images = images / 255.0\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a simple CNN model\n",
    "def build_cnn(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  # 4 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate(x_train, x_val, x_test, y_train, y_val, y_test, input_shape):\n",
    "    model = build_cnn(input_shape)\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    return history, test_acc\n",
    "\n",
    "# Run training and evaluation on Original Images\n",
    "print(\"Training on Original Images...\")\n",
    "history_original, acc_original = train_and_evaluate(\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "print(f\"Original Images Test Accuracy: {acc_original:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
